{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model means finding the weight that minimizes the loss.\n",
    "\n",
    "The expression \"argminloss(w)\" represents the value of the parameter vector \"w\" that minimizes a given loss function. This notation is often used in optimization problems to denote the argument (in this case, the parameter vector \"w\") that yields the minimum value of the specified loss function.\n",
    "\n",
    "Let's break down the components:\n",
    "\n",
    "- \"argmin\": This is short for \"argument of the minimum.\" It refers to the input value (or argument) that minimizes the function following it.\n",
    "\n",
    "- \"loss(w)\": This is the loss function that depends on the parameter vector \"w.\" The loss function quantifies the discrepancy between the model's predictions and the actual (ground truth) values. In the context of machine learning and optimization, the goal is to find the parameter values that minimize this loss function.\n",
    "\n",
    "So, \"argminloss(w)\" essentially means finding the parameter vector \"w\" that minimizes the loss function. Mathematically, it can be written as:\n",
    "\n",
    "```\n",
    "w_optimal = argmin(loss(w))\n",
    "```\n",
    "\n",
    "In practical machine learning and optimization tasks, finding the exact value of \"w\" that minimizes the loss function might involve complex mathematical computations and iterative optimization algorithms. These algorithms, such as gradient descent, stochastic gradient descent, or more advanced methods like Adam, iteratively adjust the parameter values to approach the optimal value that minimizes the loss.\n",
    "\n",
    "The process of finding the argument that minimizes a loss function is fundamental in training machine learning models, as it allows the model to learn the best parameter values that result in accurate predictions on the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*Gradient descent* is a widely used optimization algorithm in machine learning and deep learning. It's used to minimize a given objective function, typically referred to as the \"loss\" or \"cost\" function. The main idea behind gradient descent is to iteratively adjust the parameters of a model in the direction that reduces the value of the loss function, ultimately reaching a local minimum (or potentially a global minimum) of the function.\n",
    "</br>\n",
    "\n",
    "![Alt text](gd.png)\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "1. **Initialization**: Start by initializing the model's parameters randomly or with some predefined values.\n",
    "\n",
    "2. **Compute Gradient**: Calculate the gradient of the loss function with respect to each parameter. The gradient represents the direction of steepest increase of the loss function. In other words, it indicates how much the loss would increase if each parameter was increased by a small amount.\n",
    "\n",
    "    $gradient(g)=$ $∇loss(l)\\over∇weight(w)$\n",
    "\n",
    "3. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient. This means subtracting a fraction of the gradient from each parameter. The fraction of the gradient that's subtracted is determined by a parameter called the learning rate.\n",
    "\n",
    "    $w = w - α$ $∇l\\over∇w$   \n",
    "    \n",
    "    where, α = learning rate\n",
    "    \n",
    "\n",
    "For our linear model: <br>\n",
    "    $g=$ $∇l\\over∇w$ = $∇(ŷ-y)²\\over∇w$ = $∇(x*w-y)²\\over∇w$ = $2x(xw - y)$\n",
    "\n",
    "![Loss Calculation](loss.png)\n",
    "    \n",
    "4. **Iterate**: Repeat steps 2 and 3 for a certain number of iterations or until a convergence criterion is met. The goal is to iteratively move closer to the minimum of the loss function.\n",
    "\n",
    "    ***For each iteration, w will be now updated using this equation:***\n",
    "        $w = w - α*2x(xw - y)$\n",
    "\n",
    "5. **Convergence**: The algorithm continues to adjust the parameters, ideally leading to convergence, where the loss function stops decreasing significantly, and the parameter values stabilize.\n",
    "\n",
    "The choice of learning rate, batch size, and other hyperparameters can significantly influence the performance and convergence of gradient descent. Fine-tuning these hyperparameters often requires experimentation and monitoring the training process.\n",
    "\n",
    "Gradient descent forms the basis for many optimization algorithms used in training neural networks and other machine learning models. Its goal is to find the parameter values that result in the best model fit to the data and the lowest possible loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Gradient Descent\n",
    "X = [1.0,2.0,3.0,4.0] # data\n",
    "y = [2.0,4.0,6.0,8.0] # target\n",
    "\n",
    "# weight : taking a random guess\n",
    "w = 1.0\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass for our model\n",
    "def forward(x):\n",
    "  # ŷ = x * w\n",
    "  return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss function\n",
    "def loss(x,y):\n",
    "  y_pred = forward(x)\n",
    "  # loss = (ŷ-y)²\n",
    "  return (y_pred-y) * (y_pred-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "def gradient(x,y):\n",
    "    return 2*x*(x*w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : 4 -> 4.0\n"
     ]
    }
   ],
   "source": [
    "# Test before training\n",
    "print(f'Prediction before training : 4 -> {forward(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: 1.0,2.0,-2.0\n",
      "grad: 2.0,4.0,-7.84\n",
      "grad: 3.0,6.0,-16.2288\n",
      "grad: 4.0,8.0,-23.657984\n",
      "Progress: 0, w=1.4972678400000001, l=4.043833995172248\n",
      "grad: 1.0,2.0,-1.0054643199999997\n",
      "grad: 2.0,4.0,-3.9414201343999995\n",
      "grad: 3.0,6.0,-8.158739678208\n",
      "grad: 4.0,8.0,-11.89362939756544\n",
      "Progress: 1, w=1.7472603753017344, l=1.0220370862819224\n",
      "grad: 1.0,2.0,-0.5054792493965312\n",
      "grad: 2.0,4.0,-1.981478657634403\n",
      "grad: 3.0,6.0,-4.101660821303216\n",
      "grad: 4.0,8.0,-5.979309997277575\n",
      "Progress: 2, w=1.8729396625578516, l=0.2583092696146019\n",
      "grad: 1.0,2.0,-0.2541206748842968\n",
      "grad: 2.0,4.0,-0.9961530455464427\n",
      "grad: 3.0,6.0,-2.062036804281137\n",
      "grad: 4.0,8.0,-3.0059914302409467\n",
      "Progress: 3, w=1.9361226821073798, l=0.06528498785847768\n",
      "grad: 1.0,2.0,-0.12775463578524038\n",
      "grad: 2.0,4.0,-0.5007981722781416\n",
      "grad: 3.0,6.0,-1.036652216615753\n",
      "grad: 4.0,8.0,-1.5112085646665179\n",
      "Progress: 4, w=1.9678868180008364, l=0.016500103329782457\n"
     ]
    }
   ],
   "source": [
    "# Training : Adjusting weight\n",
    "# Training Loop \n",
    "for epoch in range(5):\n",
    "    for x_val,y_val in zip(X,y):\n",
    "        grad = gradient(x_val,y_val)\n",
    "        # now update weight using calculated gradient & set learning rate(alpha)\n",
    "        w = w - alpha * grad\n",
    "        print(f'grad: {x_val},{y_val},{grad}')\n",
    "        l = loss(x_val,y_val)\n",
    "\n",
    "    print(f'Progress: {epoch}, w={w}, l={l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training : 4 -> 7.871547272003346\n"
     ]
    }
   ],
   "source": [
    "# Test after training\n",
    "print(f'Prediction after training : 4 -> {forward(4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
